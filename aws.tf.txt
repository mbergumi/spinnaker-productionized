



# Specify the provider and access details
provider "aws" {
  region = "us-west-2"
  access_key = ""
  secret_key = ""
}

# Create a VPC to launch our instances into
resource "aws_vpc" "default" {
  cidr_block = "10.0.0.0/16"
}

# Create an internet gateway to give our subnet access to the outside world
resource "aws_internet_gateway" "default" {
  vpc_id = "${aws_vpc.default.id}"
}

# Grant the VPC internet access on its main route table
resource "aws_route" "internet_access" {
  route_table_id         = "${aws_vpc.default.main_route_table_id}"
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = "${aws_internet_gateway.default.id}"
}

# Create a subnet to launch our instances into
resource "aws_subnet" "default" {
  vpc_id                  = "${aws_vpc.default.id}"
  cidr_block              = "10.0.1.0/24"
  map_public_ip_on_launch = true
}

# A security group for the ELB so it is accessible via the web
resource "aws_security_group" "elb" {
  name        = "terraform_example_elb"
  description = "Used in the terraform"
  vpc_id      = "${aws_vpc.default.id}"

  # HTTP access from anywhere
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # outbound internet access
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Our default security group to access
# the instances over SSH and HTTP
resource "aws_security_group" "default" {
  name        = "terraform_example"
  description = "Used in the terraform"
  vpc_id      = "${aws_vpc.default.id}"

  # SSH access from anywhere
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # HTTP access from the VPC
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  # outbound internet access
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}


resource "aws_elb" "web" {
  name = "terraform-example-elb"

  subnets         = ["${aws_subnet.default.id}"]
  security_groups = ["${aws_security_group.elb.id}"]
  instances       = ["${aws_instance.web.id}"]

  listener {
    instance_port     = 80
    instance_protocol = "http"
    lb_port           = 8080
    lb_protocol       = "http"
  }

}

 resource "aws_key_pair" "auth" {
   key_name   = "terraform"
   public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC/aNyRkmGbaVYn6oP/I8damplsAuYqpgcWI850zqpZGZCzWkt68NRB/ALAH5J8zIImWWYJsn8dVHjIwibUSdEAxFzbabanx3R5yZT8Ydvc56iW/2rdTsFvm6Moz4Jyw6VkJVQVNAMs6uXoNukAez7VI2mA5APi+x0BC1+wlmi+PeJCSYXuYhGuZ0BZOfuUhGV6qV5ktH6DzY8f9a9HfYM2ZhYfTwj1sZNLyx5CyOn+0z1M0n7d6L7C1KyCcRtnjRI848JHezKYh+Dfn6rtBAgIik985pr/gi9GTu01npuaI1XAgBaDqN9KFKdkkSYJnGvMyeQBJQShya+slZpcG2hB0sq9dpB9+rv9yJmCrX4c24TKmDXJWMcqDCeivxMzvT+z7ttNJiX788Nt2nWEZuvHViUi9Rs1dpqFmJUChjw28dq59HDnfVxIw3LNXc1SM1IrZ419kCssO6VOX7XzXYjUmWw0TO5ObV9xtgdonHz4IlPMkw5lTOi1L6M/7YblM492dCi3TpWRr+JGk3Pl7jp+18RNPqurHLOMjPtQZNr3XZCapOztDTbGaXC2qvP1l7WVGxkjNwY6RCKkVxc7ZDDLPLLOiF00ipiJPaqXhBJ1z22A8XErrdLmptE8MHJ4sJvHFqSvny37CzWShuYlYpx/awWS8+3CPtLvIwQmFngw7Q== cmoon@kenzan.com"
 }

resource "aws_instance" "web" {
  # The connection block tells our provisioner how to
  # communicate with the resource (instance)
  connection {
    # The default username for our AMI
    user = "ubuntu"

    # The connection will use the local SSH agent for authentication.
  }

  instance_type = "t2.micro"

  # Lookup the correct AMI based on the region
  # we specified
  #ami = "${lookup(var.aws_amis, var.aws_region)}"
  ami = "ami-d732f0b7"
  # The name of our SSH keypair we created above.
  key_name = "${aws_key_pair.auth.id}"

  # Our Security group to allow HTTP and SSH access
  vpc_security_group_ids = ["${aws_security_group.default.id}"]

  # We're going to launch into the same subnet as our ELB. In a production
  # environment it's more common to have a separate private subnet for
  # backend instances.
  subnet_id = "${aws_subnet.default.id}"

  # We run a remote provisioner on the instance after creating it.
  # In this case, we just install nginx and start it. By default,
  # this should be on port 80

provisioner "file" {
        source = "config/front50.yml"
        destination = "/opt/front50.yml"
    }

  provisioner "remote-exec" {
    inline = [
      "export DEBIAN_FRONTEND=noninteractive",
      "sudo apt-get update",
      "sudo apt-get install -y git",
      "git",
      "sudo add-apt-repository -y ppa:webupd8team/java",
      "echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections",
      "sudo apt-get update",
      "sudo apt-get install -y oracle-java8-installer",
      "git clone https://github.com/spinnaker/front50.git",
      "cd front50 && GRADLE_USER_HOME=cache sudo ./gradlew buildDeb -x test",
      "sudo dpkg -i ./front50-web/build/distributions/*.deb",
      "rm /opt/front50/config/front50.yml",
      "mv /opt/front50.yml /opt/front50/config/front50.yml",
      "sudo /opt/front50/bin/front50 &> ~/front50.log"
    ]
  }
}